{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from scipy import stats as sps\n",
    "\n",
    "import pytensor.tensor as ptt\n",
    "\n",
    "import icomo\n",
    "\n",
    "plt.rcParams.update({'font.size': 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_confidence_interval(data, n=1000, func=np.mean, alpha=0.05):\n",
    "    resamples = np.random.choice(data.dropna(), size=(n, len(data.dropna())), replace=True)\n",
    "    perc = np.percentile([func(r) for r in resamples], [100*alpha/2, 100*(1-alpha/2)])\n",
    "    return [func(data.dropna()) - perc[0], perc[1] - func(data.dropna())]\n",
    "\n",
    "def percentile_confidence_interval(data, alpha=0.05):\n",
    "    perc = np.percentile(data.dropna(), [100*alpha/2, 100*(1-alpha/2)])\n",
    "    return [data.mean() - perc[0], perc[1] - data.mean()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard parameters\n",
    "_RATE_RECOVER = 1.0 / 3.0\n",
    "_RATE_LOAD = 1.0 / 0.5\n",
    "_RATE_DOCK = 1.0 / 1.0\n",
    "_RATE_PRIME = 1.0 / 0.1\n",
    "_MAX_RECOVERY = 38.0\n",
    "_MAX_LOADED = 38.0\n",
    "_MAX_DOCKED = 20.0\n",
    "_MAX_PRIMED = 4.0\n",
    "_MAX_FUSED = 100.0\n",
    "_RELEASE_PROBABILITY = 0.2\n",
    "\n",
    "# Define the differential equations\n",
    "def equations(t, y, args):\n",
    "    t_args, const_args = args\n",
    "\n",
    "    RATE_RECOVER = const_args[\"RATE_RECOVER\"]\n",
    "    RATE_LOAD = const_args[\"RATE_LOAD\"]\n",
    "    RATE_DOCK = const_args[\"RATE_DOCK\"]\n",
    "    RATE_PRIME = const_args[\"RATE_PRIME\"]\n",
    "    MAX_RECOVERY = const_args[\"MAX_RECOVERY\"]\n",
    "    MAX_LOADED = const_args[\"MAX_LOADED\"]\n",
    "    MAX_DOCKED = const_args[\"MAX_DOCKED\"]\n",
    "    MAX_PRIMED = const_args[\"MAX_PRIMED\"]\n",
    "    RELEASE_PROBABILITY = const_args[\"RELEASE_PROBABILITY\"]\n",
    "\n",
    "    stim_rate = t_args\n",
    "\n",
    "    recovery = y[\"recovery\"]\n",
    "    loaded = y[\"loaded\"]\n",
    "    docked = y[\"docked\"]\n",
    "    primed = y[\"primed\"]\n",
    "    fused = y[\"fused\"]\n",
    "\n",
    "    # recovery\n",
    "    fused_to_recovery = RATE_RECOVER * (fused > 0) * (recovery <= MAX_RECOVERY)\n",
    "    recovery_to_loaded = 4 * RATE_LOAD * (recovery / MAX_RECOVERY) * (1 - loaded / MAX_LOADED)\n",
    "    loaded_to_docked = 4 * RATE_DOCK * (loaded / MAX_LOADED) * (1 - docked / MAX_DOCKED)\n",
    "    docked_to_primed = 2 * RATE_PRIME * (docked / MAX_DOCKED) * (1 - primed / MAX_PRIMED)\n",
    "\n",
    "    drecovery = fused_to_recovery - recovery_to_loaded\n",
    "    dloaded = recovery_to_loaded - loaded_to_docked\n",
    "    ddocked = loaded_to_docked - docked_to_primed\n",
    "    dprimed = docked_to_primed\n",
    "    dfused = - fused_to_recovery\n",
    "\n",
    "    # release\n",
    "    release = RELEASE_PROBABILITY * primed * stim_rate(t) \n",
    "    dprimed -= release\n",
    "    dfused += release\n",
    "\n",
    "    dy = {\n",
    "        \"recovery\": drecovery,\n",
    "        \"loaded\": dloaded,\n",
    "        \"docked\": ddocked,\n",
    "        \"primed\": dprimed,\n",
    "        \"fused\": dfused,\n",
    "    }\n",
    "    \n",
    "    return dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit to experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get experimental data from xls\n",
    "excel_file = \"exp1_data.xlsx\" # change path to the location of your file\n",
    "df1 = pd.read_excel(excel_file)\n",
    "# print(df1)\n",
    "\n",
    "# load exhaustion experiment data from xls\n",
    "excel_file = \"exp2_data.xlsx\" \n",
    "df2 = pd.read_excel(excel_file)\n",
    "data2 = df2[df2.columns[1:]].values.T\n",
    "time = df2[\"seconds\"].values\n",
    "print(data2.shape)\n",
    "print(time.shape)\n",
    "experimental_fusion_rate = np.diff(data2) / np.diff(time)\n",
    "experimental_fusion_rate = np.concatenate([experimental_fusion_rate, np.zeros((experimental_fusion_rate.shape[0],1))], axis=1)\n",
    "print(experimental_fusion_rate.shape)\n",
    "\n",
    "# stim starts at 32.35 s\n",
    "stim_start = 32.35\n",
    "inds = time > stim_start\n",
    "time = time[inds] - time[inds][0]\n",
    "data2 = data2[:,inds]\n",
    "# start at 0 not 1\n",
    "data2 = data2 - data2[:,[0]]\n",
    "experimental_fusion_rate = experimental_fusion_rate[:,inds]\n",
    "print(data2.shape)\n",
    "print(time.shape)\n",
    "\n",
    "data2_means = np.mean(data2, axis=0)\n",
    "data2_errs = np.std(data2, axis=0) / np.sqrt(data2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pps = 10 # points per second\n",
    "\n",
    "def get_integrator_object(depletiontime, pausetime, testtime):\n",
    "    len_sim = depletiontime + pausetime + testtime # s\n",
    "    num_points = int(len_sim * pps)\n",
    "\n",
    "    t_solve_ODE = np.linspace(0, len_sim, num_points) # timepoints at which the ODE is solved\n",
    "    t_stim = t_solve_ODE # timepoints at which the stimulus is defined\n",
    "\n",
    "    # only return the output during the test time\n",
    "    t_out_inds = np.logical_and((t_solve_ODE > depletiontime + pausetime), (t_solve_ODE < depletiontime + pausetime + testtime))\n",
    "    t_out = t_solve_ODE[t_out_inds]\n",
    "\n",
    "    integrator_object = icomo.ODEIntegrator(\n",
    "        ts_out=t_out,\n",
    "        t_0=min(t_solve_ODE),\n",
    "        ts_solver=t_solve_ODE,\n",
    "        ts_arg=t_stim,\n",
    "        max_steps=len(t_solve_ODE),\n",
    "    )\n",
    "    return integrator_object, t_stim\n",
    "\n",
    "def get_integrator_object2():\n",
    "\n",
    "    len_sim = 70 # s\n",
    "    pps = 17 # points per second later scaled to 1.7 by taking mod 20\n",
    "    num_points = int(len_sim * pps) + 1\n",
    "\n",
    "    ### First set the time variables\n",
    "    t_solve_ODE = np.linspace(0, len_sim, num_points) # timepoints at which the ODE is solved\n",
    "    t_stim = t_solve_ODE # timepoints at which the stimulus is defined\n",
    "\n",
    "    inds = (np.arange(len(t_solve_ODE)) % 10 == 0) & (t_solve_ODE <= time[-1])\n",
    "    t_out = t_solve_ODE[inds] # timepoints at which the output is saved\n",
    "    assert np.all(np.isclose(t_out, time))\n",
    "\n",
    "    # First parameters of the integrators have to be set\n",
    "    integrator_object = icomo.ODEIntegrator(\n",
    "        ts_out=t_out,\n",
    "        t_0=min(t_solve_ODE),\n",
    "        ts_solver=t_solve_ODE,\n",
    "        ts_arg=t_stim,\n",
    "        max_steps=len(t_solve_ODE),\n",
    "    )\n",
    "    return integrator_object, t_stim\n",
    "\n",
    "def get_lognormal_params(mean, std):\n",
    "    sigma = np.sqrt(np.log(std**2 / mean**2 + 1))\n",
    "    mu = np.log(mean) - 0.5 * sigma**2\n",
    "    return mu, sigma\n",
    "\n",
    "with pm.Model() as model:\n",
    "\n",
    "    ###### Priors on the model parameters\n",
    "\n",
    "    # Priors on the model parameters\n",
    "    precision = 2\n",
    "    mu, sigma = get_lognormal_params(_MAX_RECOVERY, _MAX_RECOVERY/precision)\n",
    "    max_recovery = pm.LogNormal(\"max_recovery\", mu=mu, sigma=sigma)\n",
    "    mu, sigma = get_lognormal_params(_MAX_LOADED, _MAX_LOADED/precision)\n",
    "    max_loaded = pm.LogNormal(\"max_loaded\", mu=mu, sigma=sigma)\n",
    "    mu, sigma = get_lognormal_params(_MAX_DOCKED, _MAX_DOCKED/precision)\n",
    "    max_docked = pm.LogNormal(\"max_docked\", mu=mu, sigma=sigma)\n",
    "    mu, sigma = get_lognormal_params(_MAX_PRIMED, _MAX_PRIMED/precision)\n",
    "    max_primed = pm.LogNormal(\"max_primed\", mu=mu, sigma=sigma)\n",
    "    \n",
    "    precision = 0.7\n",
    "    # set all to 1\n",
    "    mu, sigma = get_lognormal_params(_RATE_DOCK, _RATE_DOCK/precision)\n",
    "    rate_recover = pm.LogNormal(\"rate_recover\", mu=mu, sigma=sigma)\n",
    "    mu, sigma = get_lognormal_params(_RATE_DOCK, _RATE_DOCK/precision)\n",
    "    rate_load = pm.LogNormal(\"rate_load\", mu=mu, sigma=sigma)\n",
    "    mu, sigma = get_lognormal_params(_RATE_DOCK, _RATE_DOCK/precision)\n",
    "    rate_dock = pm.LogNormal(\"rate_dock\", mu=mu, sigma=sigma)\n",
    "    mu, sigma = get_lognormal_params(_RATE_DOCK, _RATE_DOCK/precision)\n",
    "    rate_prime = pm.LogNormal(\"rate_prime\", mu=mu, sigma=sigma)\n",
    "    \n",
    "    release_probability1 = pm.Beta(\"release_probability1\", alpha=1, beta=2)\n",
    "    release_probability2 = pm.Beta(\"release_probability2\", alpha=1, beta=2)\n",
    "    # additional factor to scale from num vesicles to calcium data\n",
    "    observation_factor1 = pm.HalfFlat(\"observation_factor1\")\n",
    "    error_model1 = pm.HalfCauchy(\"error_model1\", beta=0.1)\n",
    "    # additional factor to scale from num vesicles to calcium data\n",
    "    observation_factor2 = pm.HalfFlat(\"observation_factor2\")\n",
    "    error_model2 = pm.HalfCauchy(\"error_model2\", beta=0.3)\n",
    "\n",
    "    const_args_var1 = {\n",
    "        \"RATE_RECOVER\": rate_recover,\n",
    "        \"RATE_LOAD\": rate_load,\n",
    "        \"RATE_DOCK\": rate_dock,\n",
    "        \"RATE_PRIME\": rate_prime,\n",
    "        \"MAX_RECOVERY\": max_recovery,\n",
    "        \"MAX_LOADED\": max_loaded,\n",
    "        \"MAX_DOCKED\": max_docked,\n",
    "        \"MAX_PRIMED\": max_primed,\n",
    "        \"RELEASE_PROBABILITY\": release_probability1,\n",
    "    }\n",
    "\n",
    "    const_args_var2 = {\n",
    "        \"RATE_RECOVER\": rate_recover,\n",
    "        \"RATE_LOAD\": rate_load,\n",
    "        \"RATE_DOCK\": rate_dock,\n",
    "        \"RATE_PRIME\": rate_prime,\n",
    "        \"MAX_RECOVERY\": max_recovery,\n",
    "        \"MAX_LOADED\": max_loaded,\n",
    "        \"MAX_DOCKED\": max_docked,\n",
    "        \"MAX_PRIMED\": max_primed,\n",
    "        \"RELEASE_PROBABILITY\": release_probability2,\n",
    "    }\n",
    "\n",
    "\n",
    "    ###### experiment 1\n",
    "\n",
    "    testtime = 2.0 # s\n",
    "    depletiontimes = [0.4, 4.0, 40.0]\n",
    "    pausetimes = [1.0, 10.0, 40.0, 100.0]\n",
    "    stim_rate = 20.0 # Hz\n",
    "    conditions = [(depletiontime, pausetime) for pausetime in pausetimes for depletiontime in depletiontimes]\n",
    "    conditions.append((40.0, 200.0))\n",
    "\n",
    "    sim = []\n",
    "    sim_error = []\n",
    "    obs = []\n",
    "\n",
    "    for condition in conditions:\n",
    "        depletiontime, pausetime = condition\n",
    "        # get condition name\n",
    "        fm = lambda x: round(x, 1) if x % 1 else int(x)\n",
    "        condition_name = \"{}_{}\".format(fm(depletiontime), fm(pausetime))\n",
    "\n",
    "        y0 = {\"recovery\": max_recovery, \"loaded\": max_loaded, \"docked\": max_docked, \"primed\": max_primed, \"fused\": 0.0}\n",
    "        \n",
    "        # define integrator object\n",
    "        integrator_object, ts = get_integrator_object(depletiontime, pausetime, testtime)\n",
    "        integrator_op = integrator_object.get_op(equations, list_keys_to_return=[\"primed\", \"fused\"], return_shapes=[() for _ in range(2)])\n",
    "        \n",
    "        # define stimulus\n",
    "        stim = ts < depletiontime\n",
    "        stim = stim + (ts > depletiontime + pausetime)\n",
    "        stim = stim * stim_rate\n",
    "\n",
    "        # And solve the ODE for our starting conditions and parameters\n",
    "        primed, fused = integrator_op(y0=y0, arg_t=stim, constant_args=const_args_var1)\n",
    "        # util = calcium + ca_increase * (1 - calcium) \n",
    "        release_rate = primed * stim_rate * release_probability1 #*util\n",
    "        released = pm.math.mean(release_rate)\n",
    "\n",
    "        # save sim data under condition name\n",
    "        pm.Deterministic(condition_name, observation_factor1 * released)\n",
    "\n",
    "        # get data and simulation results\n",
    "        data1 = df1[condition_name]\n",
    "        error_of_the_mean = np.std(data1) / np.sqrt(len(data1))\n",
    "        data1_mean = np.mean(data1)\n",
    "        sigma_error = pm.Deterministic(\"scaled_sigma_error_\" + condition_name, error_model1 + error_of_the_mean)\n",
    "        \n",
    "        sim.append(observation_factor1 * released)\n",
    "        sim_error.append(sigma_error)\n",
    "        obs.append(data1_mean)\n",
    "\n",
    "    sim = pm.math.stack(sim)\n",
    "    sim_error = pm.math.stack(sim_error)\n",
    "\n",
    "    mu1 = sim\n",
    "    sigma1 = sim_error\n",
    "    observed1 = obs\n",
    "\n",
    "\n",
    "    ###### experiment 2\n",
    "    \n",
    "    stim_rate = 5.0 # Hz\n",
    "    max_vesicles = _MAX_FUSED + max_loaded + max_docked + max_primed\n",
    "\n",
    "\n",
    "    ### Define starting conditions\n",
    "    y0 = {\"recovery\": max_recovery, \"loaded\": max_loaded, \"docked\": max_docked, \"primed\": max_primed, \"fused\": 0.0}\n",
    "\n",
    "    # Get integrator object\n",
    "    integrator_object, ts = get_integrator_object2()\n",
    "    integrator = integrator_object.get_op(equations, list_keys_to_return=[\"primed\", \"fused\"], return_shapes=[() for _ in range(2)])\n",
    "\n",
    "    ### Set input\n",
    "    stim = stim_rate * np.ones(len(ts))\n",
    "    t_args = stim\n",
    "\n",
    "    # And solve the ODE for our starting conditions and parameters\n",
    "    primed, _ = integrator(y0=y0, arg_t=t_args, constant_args=const_args_var2)\n",
    "\n",
    "    release_rate = primed * stim_rate * release_probability2\n",
    "    pps_experiment = 1.7\n",
    "    fused = pm.math.cumsum(release_rate) / pps_experiment # have to get total number of vesicles released\n",
    "    fused -= fused[0]\n",
    "\n",
    "    out = pm.Deterministic(\"experiment2\", observation_factor2 * fused)\n",
    "    sim2_errors = pm.Deterministic(\"scaled_sigma_error_experiment2\", error_model2 + data2_errs)\n",
    "\n",
    "    mu2 = out\n",
    "    sigma2 = sim2_errors\n",
    "    observed2 = data2_means\n",
    "\n",
    "    observed = np.concatenate([observed1, observed2])\n",
    "    mu = ptt.concatenate([mu1, mu2])\n",
    "    sigma = ptt.concatenate([sigma1, sigma2])\n",
    "\n",
    "    def logp(observed, mu, sigma):\n",
    "        n = 13\n",
    "\n",
    "        observed1 = observed[:n]\n",
    "        observed2 = observed[n:]\n",
    "        mu1 = mu[:n]\n",
    "        mu2 = mu[n:]\n",
    "        sigma1 = sigma[:n]\n",
    "        sigma2 = sigma[n:]\n",
    "        weight1 = 10\n",
    "        weight2 = 1\n",
    "        normalizer = weight1 + weight2\n",
    "        weight1 /= normalizer / 2\n",
    "        weight2 /= normalizer / 2\n",
    "        p1 = pm.logp(pm.Normal.dist(mu1, sigma1),observed1)\n",
    "        p2 = pm.logp(pm.Normal.dist(mu2, sigma2),observed2)\n",
    "        return ptt.concatenate([weight1 * p1, weight2 * p2])\n",
    "         \n",
    "    pm.CustomDist(\"likelihood\",\n",
    "                    mu, sigma,\n",
    "                    logp = logp,\n",
    "                    observed = observed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the model\n",
    "trace = pm.sample(\n",
    "    model=model,\n",
    "    tune=800,\n",
    "    draws=2500,\n",
    "    cores=4,\n",
    "    nuts_sampler_kwargs={\"nuts_kwargs\": {\"max_tree_depth\": 6}},\n",
    "    nuts_sampler=\"numpyro\",\n",
    "    target_accept=0.90,\n",
    ")\n",
    "\n",
    "# save the trace\n",
    "az.to_netcdf(trace, \"trace_all_same.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # map estimate\n",
    "# map_estimate = pm.find_MAP(model=model)\n",
    "# print(map_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # get the posterior\n",
    "# sim_df = pd.DataFrame()\n",
    "# for condition in conditions:\n",
    "#     depletiontime, pausetime = condition\n",
    "#     condition_name = \"{}_{}\".format(fm(depletiontime), fm(pausetime))\n",
    "#     sim_df[condition_name] = np.array([map_estimate[condition_name]])\n",
    "\n",
    "# fig = plt.figure(figsize=(3.5,2.8))\n",
    "# ax = fig.subplots(1,1)\n",
    "\n",
    "# # offset points\n",
    "# import matplotlib.transforms as transforms\n",
    "# offset = lambda p: transforms.ScaledTranslation(p/72.,0, fig.dpi_scale_trans)\n",
    "# trans = ax.transData\n",
    "\n",
    "# # remove the right and top and bottom spines\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "# errs_df = df1.apply(lambda x: bootstrap_confidence_interval(x), axis=0)\n",
    "# plt.errorbar(df1.columns, df1.mean(), yerr=errs_df, linestyle='None', marker='None', markersize=2, elinewidth=9, color='black', alpha=0.2)\n",
    "# plt.errorbar(df1.columns, df1.mean(), yerr=errs_df, linestyle='None', marker='_', markersize=9, elinewidth=0, color='black', alpha=0.2)\n",
    "\n",
    "# errs_sim_df = sim_df.apply(lambda x: percentile_confidence_interval(x), axis=0)\n",
    "# plt.plot(sim_df.columns, sim_df.mean(), linestyle='None', marker='o', color='cornflowerblue')\n",
    "\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.ylabel(\"change in dF/F\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"posterior_full_comparison.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the posteriors\n",
    "# simulation_results = np.array([map_estimate[\"experiment2\"]])\n",
    "\n",
    "# simulation_results_mean = np.mean(simulation_results, axis=0)\n",
    "# # simulation_results_conf = np.percentile(simulation_results, [2.5, 97.5], axis=0)\n",
    "\n",
    "# deviation = np.std(data2, axis=0) / np.sqrt(data2.shape[0])\n",
    "# experiment_conf = data2_means + 1.96 * np.array([-deviation, deviation])\n",
    "\n",
    "# fig, axs = plt.subplots(1,1, figsize=(2.5,2))\n",
    "# axs.plot(time, data2_means, label=\"Experimental data\", alpha=0.2, color='black')\n",
    "# axs.fill_between(time, experiment_conf[0], experiment_conf[1], alpha=0.2, color='black')\n",
    "# axs.plot(time, simulation_results_mean, label=\"Model prediction\", color='cornflowerblue')\n",
    "# # axs.fill_between(time, simulation_results_conf[0], simulation_results_conf[1], alpha=0.2, color='cornflowerblue')\n",
    "# axs.set_xlabel(\"Time [s]\")\n",
    "# axs.set_ylabel(\"Flourescence increase\")\n",
    "# axs.spines['top'].set_visible(False)\n",
    "# axs.spines['right'].set_visible(False)\n",
    "# axs.legend()\n",
    "\n",
    "\n",
    "# plt.tight_layout() \n",
    "# plt.savefig(\"exhaustion_comparison.pdf\", bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trace\n",
    "trace = az.from_netcdf(\"trace_all_same.nc\")\n",
    "# if chains do not converge throw them out\n",
    "# trace_full = trace\n",
    "# trace = trace_full.sel(chain=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(az.rhat(trace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the posterior\n",
    "sim_df = pd.DataFrame()\n",
    "for condition in conditions:\n",
    "    depletiontime, pausetime = condition\n",
    "    condition_name = \"{}_{}\".format(fm(depletiontime), fm(pausetime))\n",
    "    sim_df[condition_name] = trace.posterior[condition_name].to_numpy().flatten()\n",
    "\n",
    "fig = plt.figure(figsize=(3.5,2.8))\n",
    "ax = fig.subplots(1,1)\n",
    "\n",
    "# offset points\n",
    "import matplotlib.transforms as transforms\n",
    "offset = lambda p: transforms.ScaledTranslation(p/72.,0, fig.dpi_scale_trans)\n",
    "trans = ax.transData\n",
    "\n",
    "# remove the right and top and bottom spines\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "errs_df = df1.apply(lambda x: bootstrap_confidence_interval(x), axis=0)\n",
    "plt.errorbar(df1.columns, df1.mean(), yerr=errs_df, linestyle='None', marker='None', markersize=2, elinewidth=9, color='black', alpha=0.2)\n",
    "plt.errorbar(df1.columns, df1.mean(), yerr=errs_df, linestyle='None', marker='_', markersize=9, elinewidth=0, color='black', alpha=0.2)\n",
    "\n",
    "errs_sim_df = sim_df.apply(lambda x: percentile_confidence_interval(x), axis=0)\n",
    "plt.errorbar(sim_df.columns, sim_df.mean(), yerr=errs_sim_df, linestyle='None', marker='o', color='cornflowerblue')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"change in dF/F\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"posterior_all_same_exp1.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the posteriors\n",
    "simulation_results = trace.posterior[\"experiment2\"].to_numpy().reshape(-1, data2.shape[1])\n",
    "\n",
    "simulation_results_mean = np.mean(simulation_results, axis=0)\n",
    "simulation_results_conf = np.percentile(simulation_results, [2.5, 97.5], axis=0)\n",
    "\n",
    "deviation = np.std(data2, axis=0) / np.sqrt(data2.shape[0])\n",
    "experiment_conf = data2_means + 1.96 * np.array([-deviation, deviation])\n",
    "\n",
    "fig, axs = plt.subplots(1,1, figsize=(2.5,2))\n",
    "axs.plot(time, data2_means, label=\"Experimental data\", alpha=0.2, color='black')\n",
    "axs.fill_between(time, experiment_conf[0], experiment_conf[1], alpha=0.2, color='black')\n",
    "axs.plot(time, simulation_results_mean, label=\"Model prediction\", color='cornflowerblue')\n",
    "axs.fill_between(time, simulation_results_conf[0], simulation_results_conf[1], alpha=0.2, color='cornflowerblue')\n",
    "axs.set_xlabel(\"Time [s]\")\n",
    "axs.set_ylabel(\"Flourescence increase\")\n",
    "axs.spines['top'].set_visible(False)\n",
    "axs.spines['right'].set_visible(False)\n",
    "# axs.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.savefig(\"posterior_all_same_exp2.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cont(prior, ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    samples = pm.draw(prior, draws=1000)\n",
    "    x = np.linspace(np.min(samples), np.max(samples), 1000)\n",
    "    ax.plot(x, np.exp(pm.logp(prior,x)).eval(), color='gray')\n",
    "    return ax\n",
    "\n",
    "condition_names = [\"{}_{}\".format(fm(condition[0]), fm(condition[1])) for condition in conditions]\n",
    "\n",
    "name_dict = {\n",
    "    \"max_recovery\": r\"$P^{max}_{recovery}$\",\n",
    "    \"max_loaded\": r\"$P^{max}_{loaded}$\",\n",
    "    \"max_docked\": r\"$P^{max}_{docked}$\",\n",
    "    \"max_primed\": r\"$P^{max}_{primed}$\",\n",
    "    \"rate_recover\": r\"$r_{fused,recovery}$\",\n",
    "    \"rate_load\": r\"$r_{recovery,loaded}$\",\n",
    "    \"rate_dock\": r\"$r_{loaded,docked}$\",\n",
    "    \"rate_prime\": r\"$r_{docked,primed}$\",\n",
    "    \"release_probability1\": r\"$p^{(1)}_{fuse}$\",\n",
    "    \"release_probability2\": r\"$p^{(2)}_{fuse}$\",\n",
    "    \"observation_factor1\": r\"$\\alpha_{obs}^{(1)}$\",\n",
    "    \"observation_factor2\": r\"$\\alpha_{obs}^{(2)}$\",\n",
    "    \"error_model1\": r\"$\\sigma_{obs}^{(1)}$\",\n",
    "    \"error_model2\": r\"$\\sigma_{obs}^{(2)}$\",\n",
    "    \"rate_ca\": r\"$r_{ca}$\",\n",
    "    \"ca_increase\": r\"$\\Delta_{ca}$\",\n",
    "}\n",
    "\n",
    "# for each variable plot prior and posterior\n",
    "names = [var.name for var in model.unobserved_RVs]\n",
    "fl = lambda name: not any([condition_name in name for condition_name in condition_names]) and not \"experiment2\" in name\n",
    "names = list(filter(fl, names))\n",
    "\n",
    "sidelength = int(np.ceil(np.sqrt(len(names))))\n",
    "fig, axs = plt.subplots(sidelength, sidelength, figsize=(sidelength*2.5, sidelength*2));\n",
    "axs = axs.flatten()\n",
    "\n",
    "for name, i in zip(names, range(len(names))):\n",
    "    ax = axs[i]\n",
    "\n",
    "    pm.plot_posterior(trace, var_names=name, color='cornflowerblue', ax=ax)\n",
    "    ax.set_title(name_dict[name], fontdict={'fontsize': 15})\n",
    "    # plot prior\n",
    "    prior = model[name]\n",
    "    try:\n",
    "        plot_cont(prior, ax=ax)\n",
    "    except:\n",
    "        pass\n",
    "    # scale to posterior\n",
    "    if name == \"error_model\":\n",
    "        ax.set_xlim((0, np.max(trace.posterior[name])))\n",
    "\n",
    "plt.tight_layout();\n",
    "plt.savefig(\"posterior_all_same.pdf\", bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(az.summary(trace))\n",
    "# keys = list(const_args_var.keys())\n",
    "# keys = [k.lower() for k in keys]\n",
    "# az.plot_forest(trace, var_names=keys, combined=True, hdi_prob=0.95, transform=lambda x: np.log10(x))\n",
    "# az.plot_forest(trace, var_names=[\"error_model\"], combined=True, hdi_prob=0.95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
